{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"-LbkTrUzOJJi"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os"]},{"cell_type":"markdown","metadata":{"id":"v_Sb82o_X323"},"source":["# Load data"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":230,"status":"ok","timestamp":1721574161482,"user":{"displayName":"salvelpr","userId":"03936313709350824429"},"user_tz":-120},"id":"q63czjJGz8VQ"},"outputs":[],"source":["data = np.load(f'./CWRU_Original_data/signal_data.npy')\n","labels = np.load(f'./CWRU_Original_data/signal_data_labels.npy')"]},{"cell_type":"markdown","metadata":{"id":"jmDzaoctf82M"},"source":["# Functions"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"kBWQeBJjIA_M"},"outputs":[],"source":["def graficar_ejemplo(data):\n","   # Plot\n","    plt.figure(figsize=(20, 5))\n","    plt.plot(list(range(0,len(data))), data)\n","    plt.xlabel(\"nº de muestra\")\n","    plt.ylabel(\"Amplitud\")\n","    # plt.ylim((0,30))\n","    plt.grid()\n","    plt.show()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def graficar_todas(data):\n","     # Plot\n","    plt.figure(figsize=(20, 5))\n","    for fila in data:\n","      plt.plot(list(range(0,len(fila))), fila, 'b',alpha = 0.2)\n","    plt.xlabel(\"nº de muestra\")\n","    plt.ylabel(\"Amplitud\")\n","    # plt.ylim((0,30))\n","    plt.grid()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S9_Gz6h5v7RX"},"source":["# Original: Train 80% Test 20 %"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7q5zQbRiOglS"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split dataset to get 20% for test data\n","x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=20)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1692548459341,"user":{"displayName":"salvelpr","userId":"03936313709350824429"},"user_tz":-120},"id":"X8m0KKkyvGOI","outputId":"f0d883ea-4528-4fc4-c710-673114bb23d5"},"outputs":[{"data":{"text/plain":["(array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n"," array([223, 218, 220, 228, 232, 224, 226, 229, 214, 226]))"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Number of labels in training data\n","np.unique(y_train, return_counts=True)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1692548903179,"user":{"displayName":"salvelpr","userId":"03936313709350824429"},"user_tz":-120},"id":"uBmwvYjdw56H","outputId":"5ffb96b8-fcb1-4756-d366-9e0adc2a3dcd"},"outputs":[{"data":{"text/plain":["(array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n"," array([57, 62, 60, 52, 48, 56, 54, 51, 66, 54]))"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Number of lables in test data\n","np.unique(y_test, return_counts=True)\n"]},{"cell_type":"markdown","metadata":{"id":"G5eqSIK2ubnc"},"source":["## Unbalance original data\n","\n","### To unbalance the dataset, 200 samples are selected from label 0 and 20 samples from each of the other labels."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yrhnhZ5aulDJ"},"outputs":[],"source":["# Create dataframes\n","df_x_train = pd.DataFrame(x_train)\n","df_y_train = pd.DataFrame(y_train)\n","\n","# Insert labels column in the dataframe\n","df_x_train.insert(0, 'labels', df_y_train)\n","\n","# Change labels to int\n","df_x_train['labels'] = df_x_train['labels'].astype('int')\n","\n","grouped = df_x_train.groupby('labels')\n","\n","path_unbalanced_data = './CWRU_preprocess_data/unbalanced_dataset'\n","\n","# Create forders if they doesn´t exist\n","os.makedirs(f'{path_unbalanced_data}/unbalanced_npy_by_labels/original_data', exist_ok=True)\n","\n","for label, group in grouped:\n","  if label == 0:\n","\n","    # Create DataFrame with 200 rows of label #0\n","    df_group = group.iloc[:200,1:]\n","    np_data = df_group.to_numpy()\n","    np.save(f'{path_unbalanced_data}/unbalanced_npy_by_labels/original_data/x_train_label{label}_200samples.npy', np_data)\n","\n","    # Crear array con las etiquetas\n","    np_labels = np.zeros(200)\n","\n","  else:\n","    # For each label, take the first 20 time series\n","    df_group = group.iloc[:20,1:]\n","    np_group = df_group.to_numpy()\n","    np.save(f'{path_unbalanced_data}/unbalanced_npy_by_labels/original_data/x_train_label{label}_20samples.npy', np_group)\n","\n","    # Concatenate dataframes\n","    np_data = np.concatenate((np_data, np_group), axis=0)\n","\n","    np_nlabels = np.full(20, label)\n","    np_labels = np.concatenate((np_labels, np_nlabels), axis=None)\n","\n","np.save(f'{path_unbalanced_data}/x_train_unbalanced.npy', np_data)\n","np.save(f'{path_unbalanced_data}/y_train_unbalanced.npy', np_labels)"]},{"cell_type":"markdown","metadata":{"id":"1a6HAq2xv93w"},"source":["## Unbalanced normalized data"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"IQqFIXDrPn2O"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","x_train_norm = []\n","for fila in x_train:\n","  # Reshape for min_max_escaler\n","  fila = fila.reshape(-1, 1)\n","  # Apply min_max_scaler\n","  fila_min_max = min_max_scaler.fit_transform(fila)\n","  # Flatten vector\n","  fila_min_max = fila_min_max.flatten()\n","  # Add to array\n","  x_train_norm.append(fila_min_max)\n","\n","x_train_norm = np.array(x_train_norm)\n","\n","x_test_norm = []\n","for fila in x_test:\n","  # Reshape for min_max_escaler\n","  fila = fila.reshape(-1, 1)\n","  # Apply min_max_scaler\n","  fila_min_max = min_max_scaler.fit_transform(fila)\n","  # Flatten vector\n","  fila_min_max = fila_min_max.flatten()\n","  # Add to array\n","  x_test_norm.append(fila_min_max)\n","\n","x_test_norm = np.array(x_test_norm)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"RsMP4Ct59Rsn"},"outputs":[],"source":["# Create dataframes\n","df_x_train_norm = pd.DataFrame(x_train_norm)\n","df_y_train = pd.DataFrame(y_train)\n","\n","# Insert column labels in the dataframe\n","df_x_train_norm.insert(0, 'labels', df_y_train)\n","\n","# Change labels to int\n","df_x_train_norm['labels'] = df_x_train_norm['labels'].astype('int')\n","\n","grouped = df_x_train_norm.groupby('labels')\n","\n","# Create forders if they doesn´t exist\n","os.makedirs(f'{path_unbalanced_data}/unbalanced_npy_by_labels/normaliced_data', exist_ok=True)\n","\n","\n","for label, group in grouped:\n","  if label == 0:\n","\n","    # Create DataFrame with 200 rows of label nº0\n","    df_group = group.iloc[:200,1:]\n","    np_data = df_group.to_numpy()\n","    np.save(f'{path_unbalanced_data}/unbalanced_npy_by_labels/normaliced_data/x_train_norm_label{label}_200samples.npy', np_data)\n","\n","    # Create array with labels\n","    np_labels = np.zeros(200)\n","\n","  else:\n","    # Dataframe with the first 20 rows of each label\n","    df_group = group.iloc[:20,1:]\n","    np_group = df_group.to_numpy()\n","    np.save(f'{path_unbalanced_data}/unbalanced_npy_by_labels/normaliced_data/x_train_norm_label{label}_20samples.npy', np_group)\n","\n","    # Concatenate dataframes\n","    np_data = np.concatenate((np_data, np_group), axis=0)\n","\n","    np_nlabels = np.full(20, label)\n","    np_labels = np.concatenate((np_labels, np_nlabels), axis=None)\n","\n","np.save(f'{path_unbalanced_data}/x_train_norm_unbalanced.npy', np_data)\n","np.save(f'{path_unbalanced_data}/y_train_norm_labels_unbalanced', np_labels)"]},{"cell_type":"markdown","metadata":{"id":"cA8CHoFHxzUV"},"source":["## Unbalanced fourier data"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"31P9DeRbzZwm"},"outputs":[],"source":["from scipy.fft import fft, fftfreq\n","\n","frecuencia_muestreo = 48000\n","\n","# Fourier transform using scipy\n","x_train_fourier = []\n","for fila in x_train:\n","  # fft_result is a complex number\n","  fft_result = fft(fila)\n","  # List of frequencies from 0 to 24000 and -24000 to 0-> array([ 0., 30., 60., ..., -90., -60., -30.]) is used for plotting\n","  frecuencias = np.array(fftfreq(len(fft_result), 1/frecuencia_muestreo))\n","\n","  # Using the absolute value of the complex number\n","  fft_result_abs = np.abs(fft_result)\n","  x_train_fourier.append(fft_result_abs)\n","\n","x_train_fourier = np.array(x_train_fourier)\n","\n","\n","# Fourier transform using scipy\n","x_test_fourier = []\n","for fila in x_test:\n","  # fft_result is a complex number\n","  fft_result = fft(fila)\n","\n","  # List of frequencies from 0 to 24000 and -24000 to 0-> array([ 0., 30., 60., ..., -90., -60., -30.]) is used for plotting\n","  frecuencias = np.array(fftfreq(len(fft_result), 1/frecuencia_muestreo))\n","\n","  # Using the absolute value of the complex number\n","  fft_result_abs = np.abs(fft_result)\n","  x_test_fourier.append(fft_result_abs)\n","\n","x_test_fourier = np.array(x_test_fourier)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"SvCLNsJEXLY7"},"outputs":[],"source":["# Sort graph \n","\n","def reordenar(data):\n","  np_reordenado = []\n","  for fila in data:\n","    primera_mitad = fila[:800]\n","    segunda_mitad = fila[800:]\n","    fila_reordenada = np.concatenate((segunda_mitad, primera_mitad), axis=None)\n","    np_reordenado.append(fila_reordenada)\n","\n","  return np_reordenado\n","\n","x_train_fourier_reordenado = reordenar(x_train_fourier)\n","x_test_fourier_reordenado = reordenar(x_test_fourier)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"qTEX2mAg1r-I"},"outputs":[],"source":["# Create dataframes\n","df_x_train_fourier = pd.DataFrame(x_train_fourier_reordenado)\n","df_y_train = pd.DataFrame(y_train)\n","\n","# Insertar column labels in the dataframe\n","df_x_train_fourier.insert(0, 'labels', df_y_train)\n","\n","# Change labels to int\n","df_x_train_fourier['labels'] = df_x_train_fourier['labels'].astype('int')\n","\n","grouped = df_x_train_fourier.groupby('labels')\n","\n","# Create forders if they doesn´t exist\n","os.makedirs(f'{path_unbalanced_data}/unbalanced_npy_by_labels/fourier_data', exist_ok=True)\n","\n","for label, group in grouped:\n","  if label == 0:\n","\n","    # Create DataFrame with 200 rows of label nº0\n","    df_group = group.iloc[:200,1:]\n","    np_data = df_group.to_numpy()\n","    np.save(f'{path_unbalanced_data}/unbalanced_npy_by_labels/fourier_data/x_train_fourier_label{label}_200samples.npy', np_data)\n","\n","    # Create array with labels\n","    np_labels = np.zeros(200)\n","\n","  else:\n","    # Dataframe with first 20 rows of each label\n","    df_group = group.iloc[:20,1:]\n","    np_group = df_group.to_numpy()\n","    np.save(f'{path_unbalanced_data}/unbalanced_npy_by_labels/fourier_data/x_train_fourier_label{label}_20samples.npy', np_group)\n","\n","    # Concatenate dataframes\n","    np_data = np.concatenate((np_data, np_group), axis=0)\n","\n","    np_nlabels = np.full(20, label)\n","    np_labels = np.concatenate((np_labels, np_nlabels), axis=None)\n","\n","np.save(f'{path_unbalanced_data}/x_train_fourier_unbalanced.npy', np_data)\n","np.save(f'{path_unbalanced_data}/y_train_fourier_labels_unbalaced', np_labels)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPimNwnpnuACXGjDyxSgE+D","collapsed_sections":["v_Sb82o_X323","S9_Gz6h5v7RX","1a6HAq2xv93w","48T5msRk7JkL"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
